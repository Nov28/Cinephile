{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#단어 모델을 학습하여 다음에 나올 단어를 예측하는 걸 만들자!\n",
    "\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RNN에 쓰이는 설정들을 모아서 간단한 클래스를 만들자!\n",
    "# smallConfig, MediumConfig, Large,,,TestConfig가 있다\n",
    "# TestConfig는 모델을 검증하거나 테스트할 때 사용하고 GPU를 사용하지 않는 개인 컴퓨터에는 smallConfig을 쓰자\n",
    "class SmallConfig(object):\n",
    "    init_scale =0.1 # 가중치 행렬을 랜덤하게 초기화 할 때 생성되는 값의 범위 지정\n",
    "\n",
    "    learning_rate = 1.0 # 경사하강법에서 학습 속도를 조절하기 위한 변수 \n",
    "    lr_decay = 0.5 # 학습속도 = 학습률 * lr_decay, 학습이 반복될때마다 lr_decay는 작아짐\n",
    "                   # 소수이므로 제곱할수록 작아지기 때문\n",
    "                   # 즉, 초기에는 학습속도 빠르게 오차함수를 이동하지만, 점차 천천히 최적값에 수렴하게 됨.\n",
    "\n",
    "    max_grad_norm = 5 # 구해진 기울기(gradient)값이 과다하게 커지는것을 막는 상한 값 cf.기울기 clipping\n",
    "    \n",
    "    num_layers = 2 # RNN을 구성할 계층의 개수\n",
    "    hidden_size = 200 #한계층에 배치할 뉴런(셀)의 개수 -> 총 400뉴런 있게 됨\n",
    "\n",
    "    num_steps = 20 # RNN을 사용하여 연속적으로 처리할 데이터의 양\n",
    "                   # 즉 이 횟수만큼 셀의 가중치를 학습시킨 후, (경사하강법으로) 기울기을 업데이트 할 것.\n",
    "\n",
    "    max_max_epoch = 13 #전체학습은 13회 반복 \n",
    "    max_epoch = 4 # 그중에서 max_epoch 값이 다 커질때까지(5회까지) 초기 학습 속도 유지\n",
    "    \n",
    "    keep_prob = 1.0 # 드롭아웃 하지 않을 확률 즉 여기 small 에서는 드롭아웃을 사용하지 않는다\n",
    "    batch_size = 20 # 이 예제에서는 한번의 학습에 전체 데이터를 사용하지 않고 일부만 사용하는 미니 배치 법 사용\n",
    "                    # 20은 이 미니 batch 의 크기\n",
    "    vocab_size = 10000 # 주어진 학습 데이터 ptb.train.txt 에는 고유한 단어가 1만개 들어있다. 이를 지정해줌.\n",
    "    \n",
    "# 미리 단어 전처리에 대해서 언급\n",
    "# 단어 횟수가 높은 순으로 정렬 후 차례대로 번호 부여함.\n",
    "\n",
    "# ptb_iterator\n",
    "# 또한 학습 데이터를 배치 개수로 나누어 num_step 의 크기 만큼 나누어 읽어옴\n",
    "# 전체 학습 데이터를 batch size인 20개로 나누면 각 배치의 크기느 00000개가 되므로, [20, 00000]의 2차원 배열만든다\n",
    "# ptb_iterator는 호출될 때마다 이 2차원 배열에서 num_step 크기인 20개씩 읽어서 미니배치용 학습 데이터로 리턴. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#트레이닝과 테스트에 사용할 두개의 config 오브젝트를 만듭니다\n",
    "\n",
    "config = SmallConfig()\n",
    "eval_config = SmallConfig()\n",
    "\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PTB 모델을 만들어 주는 클래스를 작성합니다.\n",
    "class PTBModel(object):\n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "    # init에서 텐서플로를 사용하여 신경망 모델을 모두 구성\n",
    "    def __init__(self, config, is_training=False):\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_steps = config.num_steps\n",
    "        input_size = [config.batch_size, config.num_steps]\n",
    "        self.input_data = tf.placeholder(tf.int32, input_size)\n",
    "        self.targets = tf.placeholder(tf.int32, input_size)\n",
    "        \n",
    "        \n",
    "        # 텐서플로에서 제공하는 RNN 클래스 중 BasicLSTMCell 클래스를 이용\n",
    "        lstm_fn = lambda: BasicLSTMCell(config.hidden_size, forget_bias=0.0, state_is_tuple=True, \n",
    "                                        reuse=tf.get_variable_scope().reuse)\n",
    "        #여기서 state_is_tuple = true로 했는데, 이는 성능 향상을 위해 셀/은닉 상태 데이터를 튜플로 관리하게 한다.\n",
    "        \n",
    "        # SmallConfig에서는 드롭아웃이 적용되지 않습니다.\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_fn = lambda: DropoutWrapper(lstm_fn(), config.keep_prob)\n",
    "        \n",
    "        # 두개의 계층을 가진 신경망 구조를 만듭니다.\n",
    "        cell = MultiRNNCell([lstm_fn() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "        self.initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "    \n",
    "        # 모델에 입력되는 학습 데이터는 word to idx 한 [20,20] 배열 데이터. \n",
    "        # 숫자들로 단어가 나타나졌을 때, 단어의 벡터표현(word2vec)을 하기 위해서 \n",
    "        # 단어 임베딩 작업을 수행한다.\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding_size = [config.vocab_size, config.hidden_size]\n",
    "            embedding = tf.get_variable(\"embedding\", embedding_size)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        # 그래서 최종적으로 입력데이터 input은 최종적으로 [20,20,200] 행렬이 됩.\n",
    "        # [20개로 나눈 배치의 개수, num_steps만큼 각 배치에서 읽어온 데이터, 벡터값?]\n",
    "        # SmallConfig에서는 드롭아웃이 적용되지 않습니다.\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "            \n",
    "        # 입력 데이터를 한 스텝씩 셀에 주입하는 부분\n",
    "        # 각 batch 마다 순서대로 데이터를 뽑아 셀에 입력합니다. \n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            # num_steps 값만큼 입력 데이터를 순회하면서 셀 객체를 실행\n",
    "            for time_step in range(config.num_steps): \n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "            # 결국 cell에 입력되는 최종 데이터는 [20,200] 배열이 된다.\n",
    "                \n",
    "\n",
    "        # output의 크기를 20x20x200에서 400x200으로 변경합니다.\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "        softmax_w_size = [config.hidden_size, config.vocab_size]\n",
    "        \n",
    "        softmax_w = tf.get_variable(\"softmax_w\", softmax_w_size)\n",
    "        #소프트 맥스 계층의 가중치 행렬인 softmax_w 는 [200,10000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [config.vocab_size])\n",
    "        #편향 값인 b는 크기 [10000]의 행 벡터\n",
    "        \n",
    "        # 그래서 결국 소프트 맥스 계층에서 계산되는 logits의 크기는 400x10000이 됩니다.\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        \n",
    "        # sequence_loss_by_example -> 소프트 맥스 함수와 크로스 엔트로피 계산을 처리\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(self.targets, [-1])],\n",
    "            [tf.ones([config.batch_size * config.num_steps])])\n",
    "        self.cost = tf.reduce_sum(loss) / config.batch_size\n",
    "        self.final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # 기울기 클리핑을 수행합니다. config.max_grad_norm을 기준으로 클리핑\n",
    "        # 경사 하강법 최적화 클래스 객체를 형성해서 모델으 ㅣ매개변수들을 학습 시킨다.\n",
    "        # 여기서 학습되는 것은 [10000,200]크기의 단어 임베딩 행렬\n",
    "        # 각 계층마다 [400,800] 크기의 가중치 행렬과 [800]크기의 편향벡터,\n",
    "        # 소프트 맥스 계층의 [200,10000] 크기의 가중치 행렬과 [10000]크기의 편향 벡터\n",
    "        \n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "        # ??? 한 계층의 p_f, p_i, p_j, p_d를 구하는 ,,,???\n",
    "        \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 에포크를 처리할 함수를 만듭니다.\n",
    "# 이렇게 만든 텐서플로 모델을 실행시키고 셀에서 나온 은닉 상태와 셀 상태 데이터를 업데이트 하는 역할을 합니다.\n",
    "# 위 config 클래스의 max_max_epoch 만큼 반복을 진행하여 오차를 최소화 하고자 함. \n",
    "\n",
    "def run_epoch(session, m, data, is_training=False): # 텐서플로의 세션 객체, PTBModel 객체, 학습 데이터 전달받음\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    # session.run() 의 매개변수는 연산할 그래프들과 그래프에 주입할 텐서 리스트\n",
    "    # 연산할 텐서는 PTBModel 클래스의 cost, train_op, fianl_state 튜플에 담겨있는 두 계층의 셀상태와 은닉상태 데이터\n",
    "    eval_op = m.train_op if is_training else tf.no_op()\n",
    "    \n",
    "    # initial_state는 2x20x200 크기의 튜플입니다. 셀의 상태 데이터가 저장되어있고, 모델을 학습시킨 후\n",
    "    # 다음 데이터를 주입할 때 셀의 상태 데이터로 사용된다. batch 사이즈가 20이므로, 각 셀마다 20개의 상태 데이터 저장해야함\n",
    "    # 따라서 initial_state는 2개의 계층, 2종류의 상태가 반영된 [2,2] 크기의 튜플. \n",
    "    # 튜플의 각 요소는 20개의 배치, 200개의 셀이 반영된 [20,200] 크기의 텐서\n",
    "    \n",
    "    # 그래프에 주입할 텐서를 준비\n",
    "    state_list = []\n",
    "    \n",
    "    # 완전 처음에는 initial_state가 평가가 되지 않은 상태 이므로 eval메소드를 사용하여 feed_dict에 주입할 수 있도록\n",
    "    # state_list에 담아놓는다\n",
    "    for c, h in m.initial_state:\n",
    "        state_list.extend([c.eval(), h.eval()])\n",
    "    \n",
    "    # initial_state를 순회하면서 셀 상태와 은닉 상태에 각각 state_list의 값을 할당\n",
    "    \n",
    "    \n",
    "    \n",
    "    ptb_iter = reader.ptb_iterator(data, m.batch_size, m.num_steps)\n",
    "    \n",
    "    for step, (x, y) in enumerate(ptb_iter):\n",
    "       \n",
    "        fetch_list = [m.cost]\n",
    "        # final_state 튜플에 담겨있는 상태를 꺼내어 fetch_list에 담습니다. \n",
    "        for c, h in m.final_state:\n",
    "            fetch_list.extend([c, h])\n",
    "        fetch_list.append(eval_op)\n",
    "        \n",
    "        # 이전 스텝에서 구해진 state_list가 feed_dict로 주입됩니다.\n",
    "        # feed_dict에 모델의 입력데이터와 목표 데이터를 할당하고\n",
    "        feed_dict = {m.input_data: x, m.targets: y}\n",
    "        \n",
    "        # 하나의 batch 연산이 끝나면 fetch_list에 들어있는 final_state를 다음번 batch의 initial_state로 주입하기 위해\n",
    "        # state_list 변수로 전달\n",
    "        for i in range(len(m.initial_state)):\n",
    "            c, h = m.initial_state[i]\n",
    "            feed_dict[c], feed_dict[h] = state_list[i*2:(i+1)*2]\n",
    "        \n",
    "        # fetch_list에 담긴 final_state의 결과를 state_list로 전달 받습니다.\n",
    "        cost, *state_list, _ = session.run(fetch_list, feed_dict)\n",
    "\n",
    "        costs += cost\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if is_training and step % (epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                    (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                     iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = reader.ptb_raw_data('simple-examples/data')\n",
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data, valid_data, test_data 는 단어를 숫자로 바꾼 리스트입니다.\n",
    "#가장 많이 나온 단어 순으로 0번 부터 시작하여 10000번 까지의 번호를 가지고 있습니다.\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "    # 학습과 검증, 테스트를 위한 모델을 만듭니다.\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(config, is_training=True)\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(config)\n",
    "        mtest = PTBModel(eval_config)\n",
    "        \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(config.max_max_epoch):\n",
    "        # lr_decay는 반복속도를 조절해 주는 역할을 합니다.\n",
    "        lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        perplexity = run_epoch(session, m, train_data, is_training=True)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, perplexity))\n",
    "\n",
    "        perplexity = run_epoch(session, mvalid, valid_data)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, perplexity))\n",
    "\n",
    "    perplexity = run_epoch(session, mtest, test_data)\n",
    "    print(\"Test Perplexity: %.3f\" % perplexity)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
